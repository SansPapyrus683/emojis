{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanspapyrus683/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import the dataset and loader from data_utils.py\n",
    "import data_utils\n",
    "\n",
    "image_folder_path = \"data/image\"\n",
    "desired_image_size = (64, 64)\n",
    "\n",
    "batch_size = 1\n",
    "# create a dataset so that dataset[i] returns the ith image\n",
    "rl_data = data_utils.EmojiDataset(image_folder_path, desired_image_size)\n",
    "# make a dataloader that returns the images as batches for parallel processing\n",
    "rl_loader = data.DataLoader(rl_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import models\n",
    "\n",
    "\n",
    "# functions that save and load the model and optimizer\n",
    "save_to = \"./checkpoints/model.pt\"\n",
    "def save(path, gen, disc, op_g, op_d):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"generator_weights\" : gen.state_dict(),\n",
    "            \"discriminator_weights\" : disc.state_dict(),\n",
    "            \"generator_optimizer_weights\" : op_g.state_dict(),\n",
    "            \"discriminator_optimizer_weights\" : op_d.state_dict(),\n",
    "        },\n",
    "        path\n",
    "    )\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    # initialize \n",
    "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(path)\n",
    "    gen = models.Generator().to(dev)\n",
    "    disc = models.Discriminator().to(dev)\n",
    "\n",
    "    op_d = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    op_g = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "    gen.load_state_dict(checkpoint[\"generator_weights\"])\n",
    "    disc.load_state_dict(checkpoint[\"discriminator_weights\"])\n",
    "    op_g.load_state_dict(checkpoint[\"generator_optimizer_weights\"])\n",
    "    op_d.load_state_dict(checkpoint[\"discriminator_optimizer_weights\"])\n",
    "\n",
    "    return gen, disc, op_g, op_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "generator = models.Generator()\n",
    "discriminator = models.Discriminator()\n",
    "\n",
    "# use the gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# set a learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Setup optimizers for both generator and discriminator\n",
    "optim_d = torch.optim.AdamW(generator.parameters(), lr=lr)\n",
    "optim_g = torch.optim.AdamW(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407ad4cb4cab4c8f853ee177a51d318e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Discriminator' object has no attribute 'conv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/sanspapyrus683/GitHub/emojis/main.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Forward pass real batch through D\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m optim_d\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m logits \u001b[39m=\u001b[39m discriminator(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Calculate loss on all-real batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Calculate gradients for D in backward pass\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/sanspapyrus683/GitHub/emojis/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Check how the generator is doing by saving G's output on fixed_noise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/emojis/models.py:35\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m---> 35\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(image))\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Discriminator' object has no attribute 'conv'"
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    for i, data in enumerate(rl_loader):\n",
    "        ########################################################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        #######################################################\n",
    "        ## Train with all-real batch\n",
    "        # Format batch\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        optim_d.zero_grad()\n",
    "        logits = discriminator(data)\n",
    "\n",
    "        # Calculate loss on all-real batch\n",
    "        loss = criterion(logits, torch.tensor(real_label))\n",
    "\n",
    "        # Calculate gradients for D in backward pass\n",
    "        loss.backward()\n",
    "        optim_d.step()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        fake_amt = 1\n",
    "        latent_vec = torch.randn(fake_amt, 64, 100, 4, device=device)\n",
    "\n",
    "        # Generate fake image batch with G\n",
    "        optim_g.zero_grad()\n",
    "        fake_img = generator(latent_vec)\n",
    "\n",
    "        # Classify all fake batch with D\n",
    "        optim_g.zero_grad()\n",
    "        logits = discriminator(fake_img)\n",
    "\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        loss = criterion(logits, torch.tensor(fake_label))\n",
    "\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "\n",
    "        # Update D\n",
    "\n",
    "        ########################################################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        #######################################################\n",
    "        \n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "\n",
    "        # Calculate G's loss based on this output\n",
    "\n",
    "        # Calculate gradients for G\n",
    "\n",
    "        # Update G\n",
    "\n",
    "        # # Output training stats\n",
    "       \n",
    "        # Save Losses for plotting later\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images from the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
